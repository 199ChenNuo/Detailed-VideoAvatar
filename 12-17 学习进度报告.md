# 12/17 学习进度报告



---

## 项目环境搭建 ##
根据[videoavatars][1]中的指引配置即可，需要下载模型和数据集

## Step1 - Pose Reconstruction ##
   

### 1. 使用到的库简介 ###

OpenCV - 有很多图像处理和计算机视觉有关的函数

        import cv2 


命令行解析和数据存储

    import h5py
    import argparse
    import cPickle as pkl

求导、微分、积分。。很多很多很强大的功能

    import numpy as np
    import chumpy as ch
   
  openDR
  
    from opendr.camera import ProjectPoints
    from opendr.lighting import LambertianPointLight
    from opendr.renderer import ColoredRenderer
    from opendr.filters import gaussian_pyramid
    
其他

    ...

### 2. 入口函数 ###

        if __name__ == '__main__':
            parser = argparse.ArgumentParser()
            //这部分是在解析命令行中的输入参数，可以看help中的相关注释
            parser.add_argument(
            parser.add_argument(
                ... ...
            parser.add_argument(
       
            main(
            args.keypoint_file,//关节点文件，是预先通过OpenPose框架处理获取得到的，里面记录了人的18个关节点的位置坐标参考数据
            args.masks_file, //论文中提到的 silhouette image ，就是将每一帧中的人的形象截取进行二值化处理得到的图片，论文中有示意图
            args.camera,//相机文件
            args.out, //存储文件夹
            args.model, //在整个过程中被不断refine的基本人体模型
            args.prior, //这个是论文中提到的A-pose prior，但是具体是什么我没有理解到，姿势参考吗？。。。
            args.resize,//这个参数被用在keypoint_file的reshape了
            args.body_height, //论文中提到的身高
            args.nohands, //这个应该是个bool值，指计算的时候考不考虑手，不考虑的话计算关节点有关的数据时就会忽略手的数据
            args.display//这个应该就是跟展示有关的了)

keypoint_file可以看下这个[Github 项目 - OpenPose 关键点输出格式][2]

### 3. main函数 ###

        def main(keypoint_file, masks_file, camera_file, out, model_file, prior_file, resize, body_height, nohands,display):

(1) 加载数据
把预处理好的各种pkl和h5py数据加载进来
    
        model_data = pkl.load(fp)
        camera_data = pkl.load(fp)
        prior_data = pkl.load(fp)
            ...
        keypoints = h5py.File(keypoint_file, 'r')['keypoints']
        masks = h5py.File(masks_file, 'r')['masks']
        num_frames = masks.shape[0]

（2）初始化

        //将基本模型数据加载到一个Smpl对象里面
        base_smpl = Smpl(model_data)
        base_smpl.trans[:] = np.array([0, 0, 3])
        base_smpl.pose[0] = np.pi
        base_smpl.pose[3:] = prior_data['mean']//我觉得这一步可能就是把pose预设成A-pose prior的pose，从而在一个比较合理的基础上进一步改进
    
        //以下都是调用openDR里面的各种函数，我查了一下似乎是个跟openGL有点类似的库，我看着就有点怕就没细看了。。。
        camera = ProjectPoints(...)
        frustum = {...}
     ...
(3) 创建frame对象的工具函数

    # generic frame loading function
    def create_frame(i, smpl, copy=True):
        f = FrameData()
        //存储了这一帧中的模型、相机、关节点、投影（？）、剪影等各种数据
        f.smpl =
        f.camera = 
        
这里我根据前后代码，暂时判断f.keypoint是一个shape（18,3）的数组，第i项是关节点i的x，y，scores，具体第几项是哪个关节的图我发群里了

        f.keypoints = np.array(keypoints[i]).reshape(-1, 3) * np.array([resize, resize, 1])
        //这两个不懂
        f.J = joints_coco(f.smpl)
        f.J_proj = ProjectPoints(v=f.J, t=camera.t, rt=camera.rt, c=camera.c, f=camera.f, k=camera.k)
        f.mask = 
        //以下也不是很清楚
        f.collision_obj = collision_obj(f.smpl, regs)
        f.pose_prior_obj = pose_prior_obj(f.smpl, prior_data)
        f.pose_obj = (f.J_proj - f.keypoints[:, :2]) * f.keypoints[:, 2].reshape(-1, 1)

        return f
（4）挑选5个帧的数据对模型进行一个预处理

    base_frame = create_frame(0, base_smpl, copy=False)//用最开始的一帧当做base

从所有帧里面等距挑选出5个来，组成init_frames数组

    num_init = 5
    indices_init = np.ceil(np.arange(num_init) * num_frames * 1. / num_init).astype(np.int)

    init_frames = [base_frame]
    for i in indices_init[1:]:
        init_frames.append(create_frame(i, base_smpl))
使用这5帧来预处理

        init(init_frames, body_height, b2m, debug_rn)

（5）逐帧对模型进行refine

    # get pose frame by frame
    with h5py.File(out, 'w') as fp:
        last_smpl = None
        poses_dset = 
        trans_dset = 
        betas_dset = 
        //以上是要写到out里的数据
        
        for i in xrange(num_frames):
           ......

re-init部分在论文中提到，每次循环都以前一帧的pose为基，当object error过大的时候，就进行re-init

            # re-init if necessary
            reinit_frame(current_frame, prior_data['mean'], nohands, debug_rn)
核心的fit函数

            # final fit
            fit_pose(current_frame, last_smpl, frustum, nohands, debug_rn)

存储每一帧相关的数据

            poses_dset[i] = current_frame.smpl.pose.r
            trans_dset[i] = current_frame.smpl.trans.r

            if i == 0:
                betas_dset[:] = current_frame.smpl.betas.r

            last_smpl = current_frame.smpl

### 4. init函数  ###
    def init(
    frames,//用来当做init数据源的帧数组
    body_height, //输入的身高数据，可能为空（None）
    b2m, //这个是在main中从asset里直接读取的参数，被用在了身高预测里面，我不是很懂
    viz_rn //从openDR里调用的coloredrenderer对象，在get_cb中被使用，我没有弄懂):
从论文中来看，betas就是最后用于SMPL建模的shape参数，在函数中，貌似是通过对与betas有关的损失进行局部优化，从而对pose那些的数据进行初步的调整，感觉类似于神经网络里面更新w让loss变小
    
    betas = frames[0].smpl.betas
我初步认为E_开头的变量，就是类似于loss的东西，最后对它们求导来优化参数

    E_height = None
    if body_height is not None:
         E_height = height_predictor(b2m, betas) - body_height * 1000.
（1） 首先单独为每一帧确定一个大概的pose

    # first get a rough pose for all frames individually
    for i, f in enumerate(frames):
这里是对第0,2,5,8,11号关节的scores求和，scores是指这个关节定位的可信度，意思应该是要五个关节（头，左右肩，左右胯）可信度总分>3的时候，才处理

        if np.sum(f.keypoints[[0, 2, 5, 8, 11], 2]) > 3.:

这里的【2,0】和【5,0】指的是左右肩的x坐标

            if f.keypoints[2, 0] > f.keypoints[5, 0]:#left & right shouder
                f.smpl.pose[0] = 0
                f.smpl.pose[2] = np.pi
对比main中对base_smpl的pose初始化

    base_smpl.pose[0] = np.pi
    base_smpl.pose[3:] = prior_data['mean']
可以看到是pose【0】和pose【2】分别是pi和0，但是从smpl.py中看，pose整体是个shape（72）的数组，所以具体他的数据与他表示的pose怎么对应，还需要再理解一下，我暂时不清楚

            E_init = {
                'init_pose_{}'.format(i): f.pose_obj[[0, 2, 5, 8, 11]]
            }

            x0 = [f.smpl.trans, f.smpl.pose[:3]]

            if E_height is not None and i == 0:
                E_init['height'] = E_height
                E_init['betas'] = betas
                x0.append(betas)

            ch.minimize(
                E_init,
                x0,
                method='dogleg',
                options={
                    'e_3': .01,
                },
                callback=get_cb(viz_rn, f)
            )
这里主要是要提一下ch.minimize函数，根据肖老师发的chumpy文档，minimize是个提供局部优化的函数，比如对于f(x,y),minimize(f,[x,y],method="xxx")就是通过求f关于x,y的偏导（？）然后使用method策略调整x,y使f最小。这个过程我是类比神经网络里面反向传播更新参数使loss最小来理解的。    
所以再看代码，这里就是调整x0中的f.smpl.trans, f.smpl.pose来使E_init最小，‘dogleg’策略似乎是在原数据上更新。但是有一点我不太明白的就是这里E_init是一个字典，里面有很多项，这个是指里面的每一项都要最小吗？我不太懂

（2）使用不同权重再次优化

    weights = zip(
        [5., 4.5, 4.],
        [5., 4., 3.]
    )
    //相当于【（5.，5.，），（4.5，4.），（4.，3.）】
    //定义3组（w_prior, w_betas）权重


    E_betas = betas - betas.r

    for w_prior, w_betas in weights:
        x0 = [betas]

        E = {
            'betas': E_betas * w_betas,
        }
    ........//定义E

        ch.minimize(
            E,
            x0,
            method='dogleg',
            ...
        )

总而言之，通过init，对betas，pose和trans都进行了初步的优化

### 5.reinit 函数 ###

    def reinit_frame(
    frame,//1帧数据
    null_pose，//A-pose prior的mean？
    nohands,
    viz_rn):

先判断object_error是不是过大

    if (np.sum(frame.pose_obj.r ** 2) > 625 or np.sum(frame.pose_prior_obj.r ** 2) > 75)\
            and np.sum(frame.keypoints[[0, 2, 5, 8, 11], 2]) > 3.:

第一步与init的第一步一样，略过
        
        x0 = [frame.smpl.pose[:3], frame.smpl.trans]

        ...

        ch.minimize(
            E,
            x0,
            ...
        )
第二步也是局部优化，用到的GMOf是来的robustifier文件，应该是属于作者他们自己进行的可信度优化；另外就是nohands的值影响了考不考虑手的关节点数据，这个之前提到过。

        E = {
            'pose': GMOf(frame.pose_obj, 100),
            'prior': frame.pose_prior_obj * 8.,
        }

        x0 = [frame.smpl.trans]

        if nohands:
           ....

        ch.minimize(
            E,
            x0,
            method='dogleg',
            ...
        )

### 6. fit_pose 函数 ###

    def fit_pose(
    frame, 
    last_smpl, //上一帧优化后的模型，论文中提到，为了得到更好地连贯性，选择在上一帧的基础上进行进一步的优化
    frustum, nohands, viz_rn):

这里定义了faces，但是似乎没用到？

    if nohands:
        faces = faces_no_hands(frame.smpl.f)
    else:
        faces = frame.smpl.f

这里是在对观察到的剪影数据进行 distance transform
   
    dst_type = cv2.cv.CV_DIST_L2 if cv2.__version__[0] == '2' else cv2.DIST_L2

    dist_i = cv2.distanceTransform(np.uint8(frame.mask * 255), dst_type, 5) - 1
    dist_i[dist_i < 0] = 0
    dist_i[dist_i > 50] = 50
    dist_o = cv2.distanceTransform(255 - np.uint8(frame.mask * 255), dst_type, 5)
    dist_o[dist_o > 50] = 50
这里是在读取基本模型的 silhouette image 

    rn_m = ColoredRenderer(camera=frame.camera, v=frame.smpl, f=faces, vc=np.ones_like(frame.smpl), frustum=frustum,
                           bgcolor=0, num_channels=1)

这里注意‘mask’项就是论文中提到的 ‘a silhouette term’，即公式（4）；‘2dpose’应该就是指‘ state of the art 2D joint detections ’；‘Prior’应该是‘ a single-modal A-pose prior’；感觉也算对应上了论文

    E = {
        'mask': gaussian_pyramid(rn_m * dist_o * 100. + (1 - rn_m) * dist_i, n_levels=4, normalization='size') * 80.,
        '2dpose': GMOf(frame.pose_obj, 100),
        'prior': frame.pose_prior_obj * 4.,
        'sp': frame.collision_obj * 1e3,
    }
使用上一帧的数据作参考，以保持连贯性

    if last_smpl is not None:
        E['last_pose'] = GMOf(frame.smpl.pose - last_smpl.pose, 0.05) * 50.
        E['last_trans'] = GMOf(frame.smpl.trans - last_smpl.trans, 0.05) * 50.
其他的类似之前

    if nohands:
        ...

    ch.minimize(
        E,
        x0,
        method='dogleg',
        ....
    )
    


  [1]: https://github.com/thmoa/videoavatars
  [2]: https://www.aiuai.cn/aifarm712.html