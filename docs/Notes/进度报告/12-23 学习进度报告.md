# 第一篇代码部分   
1. step3 texture  
  这一步，修改了等式7（能量项），使得能够连续得处理帧。  
  每一步的初始化是上一帧，并且根据临近的帧来做regularize。  
  为了生成纹理，他们将模型变形，然后将图片的颜色反投影到可见的vertice上，  
  最后计算在所有view（我理解为各关键帧的观测）中，最正交的纹素的中值。  
  将这些纹素merge到一起就得到了最终的texture


   1. 入口函数  
```python
if __name__ == '__main__':
    parser = add_argument(
        # 输入各种参数
    )

    main(
        args.consensus, # 包含consensus的pkl文件，根据.sh文件看，应该就是第二步输出的consensus.pkl
        args.camera, # 包含camera设置的pkl文件
        args.video, # video文件
        args.pose_file, # 包含pose的文件
        args.mask_file, # 包含segmentation的文件
        args.out, # 输出文件位置
        args.model, # SMPL模型文件
        # 这个resolution在下面也经常被用到，也许是有多少片纹素？或者说是多少个被特殊标记的身体的某个位置
        args.resolution, # 输出的resolution， 默认为1000
        args.num, # 使用的关键帧数量，默认120（对应论文里面的K）
        args.first_frame, # 关键帧起点，默认0
        args.last_fram, # 关键帧终点，默认2000
        args.display # 是否做可视化处理
        )
```


    2. main函数
```python
    # 首先从文件中导入数据(model, camera, consensus)
    with open(model_file, 'rb') as fp:
        model_data = pkl.load(fp)

    with open(camera_file, 'rb') as fp:
        camera_data = pkl.load(fp)

    with open(consensus_file, 'rb') as fp:
        consensus_data = pkl.load(fp)

    pose_data = h5py.File(pose_file, 'r')
    poses = pose_data['pose'][first_frame:last_frame]
    trans = pose_data['trans'][first_frame:last_frame]
    masks = h5py.File(masks_file, 'r')['masks'][first_frame:last_frame]
    num_frames = masks.shape[0]
    indices_texture = np.ceil(np.arange(num) * num_frames * 1. / num).astype(np.int)

    vt = np.load('assets/basicModel_vt.npy')
    ft = np.load('assets/basicModel_ft.npy')
```

```python
    # 然后进行初始化
    base_smpl = Smpl(model_data)
    base_smpl.betas[:] = consensus_data['betas']
    base_smpl.v_personal[:] = consensus_data['v_personal']

    bgcolor = np.array([1., 0.2, 1.])
    iso = Isomapper(vt, ft, base_smpl.f, resolution, bgcolor=bgcolor)
    iso_vis = IsoColoredRenderer(vt, ft, base_smpl.f, resolution)
    camera = ProjectPoints(t=camera_data['camera_t'], rt=camera_data['camera_rt'], c=camera_data['camera_c'],
                           f=camera_data['camera_f'], k=camera_data['camera_k'], v=base_smpl)
    frustum = {'near': 0.1, 'far': 1000., 'width': int(camera_data['width']), 'height': int(camera_data['height'])}
    rn_vis = ColoredRenderer(f=base_smpl.f, frustum=frustum, camera=camera, num_channels=1)

    # 打开视频文件
    cap = cv2.VideoCapture(video_file)
    for _ in range(first_frame):
        # 获得下一帧
        cap.grab()
```

```python
    # get part-textures
    i = first_frame
    # tex_agg应该是包含所有纹素的列表
    tex_agg = np.zeros((resolution, resolution, 25, 3))
    tex_agg[:] = np.nan
    # normal_agg从字面意思来理解的话，估计是包含所有法向量的列表？
    normal_agg = np.ones((resolution, resolution, 25)) * 0.2

    vn = VertNormals(f=base_smpl.f, v=base_smpl)
    static_indices = np.indices((resolution, resolution))
```

```python
    while cap.isOpened() and i < indices_texture[-1]:
        if i in indices_texture:
            log.info('Getting part texture from frame {}...'.format(i))
            # grab下一帧并且返回，这里的'_'应该只是一个表示read()是否成功的布尔值， frame就是下一帧
            _, frame = cap.read()

            mask = np.array(masks[i], dtype=np.uint8)
            pose_i = np.array(poses[i], dtype=np.float32)
            trans_i = np.array(trans[i], dtype=np.float32)

            base_smpl.pose[:] = pose_i
            base_smpl.trans[:] = trans_i

            # (源码中自带的注释）which faces have been seen and are projected into the silhouette?
            # 我的理解是，这里只考虑会被看到的面，对应论文中讲到的，把color of image反投影到**可见**的vertice上
            visibility = rn_vis.visibility_image.ravel()
            visible = np.nonzero(visibility != 4294967295)[0]

            # 这一段没怎么看懂
            proj = camera.r
            in_viewport = np.logical_and(
                np.logical_and(np.round(camera.r[:, 0]) >= 0, np.round(camera.r[:, 0]) < frustum['width']),
                np.logical_and(np.round(camera.r[:, 1]) >= 0, np.round(camera.r[:, 1]) < frustum['height']),
            )
            in_mask = np.zeros(camera.shape[0], dtype=np.bool)
            idx = np.round(proj[in_viewport][:, [1, 0]].T).astype(np.int).tolist()
            in_mask[in_viewport] = mask[idx]

            faces_in_mask = np.where(np.min(in_mask[base_smpl.f], axis=1))[0]
            visible_faces = np.intersect1d(faces_in_mask, visibility[visible])

            # get the current unwrap
            # 用iso.render()处理了一下当前帧、可见的面
            part_tex = iso.render(frame / 255., camera, visible_faces)

            # angle under which the texels have been seen
            points = np.hstack((proj, np.ones((proj.shape[0], 1))))
            points3d = camera.unproject_points(points)
            points3d /= np.linalg.norm(points3d, axis=1).reshape(-1, 1)
            alpha = np.sum(points3d * -vn.r, axis=1).reshape(-1, 1)
            alpha[alpha < 0] = 0
            iso_normals = iso_vis.render(alpha)[:, :, 0]
            iso_normals[np.all(part_tex == bgcolor, axis=2)] = 0

            # texels to consider
            # 这里是意思是，找到需要处理的纹素？这一步和上一步（一个纹素在哪些角度下能被看到）有关
            part_mask = np.zeros((resolution, resolution))
            min_normal = np.min(normal_agg, axis=2)
            part_mask[iso_normals > min_normal] = 1.

            # update best seen texels
            # 我的理解是，把这个纹素更新到这一步得到的，最优情况
            where = np.argmax(np.atleast_3d(iso_normals) - normal_agg, axis=2)
            idx = np.dstack((static_indices[0], static_indices[1], where))[part_mask == 1]
            tex_agg[list(idx[:, 0]), list(idx[:, 1]), list(idx[:, 2])] = part_tex[part_mask == 1]
            normal_agg[list(idx[:, 0]), list(idx[:, 1]), list(idx[:, 2])] = iso_normals[part_mask == 1]

            if display:
                im.show(part_tex, id='part_tex', waittime=1)

        else:
            cap.grab()

        i += 1
```
```python
    # merge textures
    log.info('Computing median texture...')
    # 这里应该是寻找，“最正交的纹素的中值”
    tex_median = np.nanmedian(tex_agg, axis=2)

    log.info('Inpainting unseen areas...')
    where = np.max(normal_agg, axis=2) > 0.2

    tex_mask = iso.iso_mask
    mask_final = np.float32(where)

    kernel_size = np.int(resolution * 0.02)
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    inpaint_area = cv2.dilate(tex_mask, kernel) - mask_final

    tex_final = cv2.inpaint(np.uint8(tex_median * 255), np.uint8(inpaint_area * 255), 3, cv2.INPAINT_TELEA)

    cv2.imwrite(out, tex_final)
    log.info('Done.')
```

# 问题，注意的点  
1. 第一篇：  
    1. 代码中出现的 face 应该是指面（与点相对应），而不是脸（与手像对应）  

    1. global transformation到底是什么

    1. 在paper1中 选取F=120个关键帧来进行迭代。在paper2中的shape-from-shading, 改为了K=60  

    1. nohands为真时会少考虑4个关节点（对应：20-23号关节点，分别是左右手腕和手）  
    这样会影响faces, rn_m的计算（rn_m是'silhouette image of the rendered model'）  
    相应的代码：
    ```python
    # 下面这段代码在fit_pose()中

    # 得到faces
    if nohands:
        faces = faces_no_hands(frame.smpl.f)
    else:
        faces = frame.smpl.f
    ...
    # 计算rn_m
    rn_m = ColoredRenderer(camera=frame.camera, v=frame.smpl, f=faces, vc=np.ones_like(frame.smpl), frustum=frustum,
                           bgcolor=0, num_channels=1)
    ...
    E = {
        'mask': gaussian_pyramid(rn_m * dist_o * 100. + (1 - rn_m) * dist_i, n_levels=4, normalization='size') * 80.,
        '2dpose': GMOf(frame.pose_obj, 100),
        'prior': frame.pose_prior_obj * 4.,
        'sp': frame.collision_obj * 1e3,
    }
    ```
    各关节：
    ```python
    self.j_names = {  
        0: 'Pelvis',  
        1: 'L_Hip',  2: 'R_Hip',   
        3:  'Spine1',  
        4: 'L_Knee',  5:  'R_Knee',   
        6:  'Spine2',    
        7:  'L_Ankle', 8:  'R_Ankle',    
        9:  'Spine3',  10: 'L_Foot',   
        11: 'R_Foot',    
        12: 'Neck',    
        13: 'L_Collar',  14: 'R_Collar',    
        15: 'Head',   
        16: 'L_Shoulder', 17: 'R_Shoulder',    
        18: 'L_Elbow',  19: 'R_Elbow',   
        20: 'L_Wrist',  21: 'R_Wrist',           
        22: 'L_Hand',  23: 'R_Hand',   
    }  
    ```
    ![joints](https://github.com/noahcao/Detailed-Human-Avatars-from-Monocular-Video/blob/master/assets/SMPL_24joint.png)
)
1. 代码与论文中的对应关系  
    1. 等式4：
    代码：  
    ```python
    'mask': gaussian_pyramid(rn_m * dist_o * 100. + (1 - rn_m) * dist_i, n_levels=4, normalization='size') * 80.,
    ```
    等式：  
    ![equal4](https://github.com/noahcao/Detailed-Human-Avatars-from-Monocular-Video/blob/master/assets/equal4.png)  
    
    1. SMPL的offset： D  
    ```python
    v_personal
    ```




